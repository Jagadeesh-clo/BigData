1. What is the definition of Hive? What is the present version of Hive? 
Ans: Hive is a Data Warehouse system which is used to analyze the structured data and build on top of Hadoop. 
Hive mainly used to manage the data using SQL queries named as HQL ( Hive query language). It's main functionalities includes reading, writing and managing large datasets residing in distributed storage. 
Using Hive, we can skip the requirement of the traditional approach of writing complex MapReduce programs. Hive supports, DDL,DML and User defined functions.

Version -- Apache hive 3.1.2 ( released on april 2022 )

2. Is Hive suitable to be used for OLTP systems? Why?
Ans; No, hive does not support insert and update at row level. So it does not support OLTP system. 

3. How is HIVE different from RDBMS? Does hive support ACID 
transactions. If not then give the proper reason.

RDBMS is a subset of DBMS. A relational database refers to a database that stores data in a structured format using rows and columns and that structured form is known as table.
Hive is a data warehouse software system that provides data query and analysis. 
RDBMS used to maintain databases and hive used to maintain the warehouses. Schema is fixed in RDBMS and schema will vary in Hive.

Hive supports all ACID properties which enable atomicity of operations at the row level, which allows a Hive client to read from a partition or table and simultaneously, another Hive client can add rows to the same partition or table.

4. Explain the hive architecture and the different components of a Hive architecture?

Hive architecture is made up of below components.
* Hive Client ( Thrift Server, JDBC Server,ODBC server)
* Hive Services ( Hive Driver -->MetaStore )
* Processing and resource management ( MapReduce ) 
* HDFS - Distributed storage system.

5. Mention what Hive query processor does? And Mention what are the 
components of a Hive query processor?

Ans: Hive query processor convert graph of MapReduce jobs with the execution time framework. So that the jobs can be executed in the order of dependencies.

Components of a Hive Query Processor:

Parse and Semantic Analysis (ql/parse)
Metadata Layer (ql/metadata)
Type Interfaces (ql/typeinfo)
Sessions (ql/session)
Map/Reduce Execution Engine (ql/exec)
Plan Components (ql/plan)
Hive Function Framework (ql/udf)
Tools (ql/tools)
Optimizer (ql/optimizer)

6. What are the three different modes in which we can operate Hive?

Hive has three different modes.
1. Local Mode - In Hive local mode, Map Reduce jobs related to Hive run locally on a user machine. This is the default mode in which Hadoop uses local file system.
2. MapReduce mode or distributed mode - Hive as well as Hadoop is running in a fully distributed mode. NameNode, DataNode, JobTracker, TaskTracker etc run on different machines in this mode.
3. Pseudo distributed mode. - This is the mode used by developers to test the code before deploying to production. In this mode, all the daemons run on same virtual machine. With this mode, we can quickly write scripts and test on limited data sets.

 7. Features and Limitations of Hive.

Ans: 

Features of Hive:
* Open-source: Apache Hive is an open-source tool. We can use it free of cost.
* Query large datasets: Hive can query and manage huge datasets stored in Hadoop Distributed File System.
* Multiple-users: Multiple users can query the data using Hive Query Language simultaneously.
* Apache Hive supports partitioning and bucketing of data at the table level to improve performance.
* Apache Hive supports external tables. This allows us to process data without actually storing data in HDFS.

Limitations of Hive:
* Hive is not designed for the OLTP (Online transaction processing). We can use it for OLAP.
* It does not offer real-time queries.
* It provides limited subquery support.
* Latency of hive is generally high.
* Hive uses HIVE query language to query structure data which is easy to code.

8. How to create a Database in HIVE?

Ans: Inorder to create a database in hive, we should open the Hive shell.
* We will be using the HQL ( Hive query language) in hive shell.
Syntax: create database databasename;
eg: create database hive_class;

9. How to create a table in HIVE?
Ans: Open the hiveshell and enter the below code to create the tables.

Query: Use database hive_class;

create table sampletable1
(
id int,
name string,
location string
)
row format delimited
fields terminated by ",";

#loading data into the table:

load data local inpath "file:///path_____" into table sampletable1;

10.What do you mean by describe and describe extended and describe 
formatted with respect to database and table

Describe:
Describe statement gives the basic information such column names and it's data types of a table.
eg. describe sampletable1;

describe extended:
describe extended gives the detailed table information such as schema of the table,warehouse info, db info etc.

Describe formatted:

Describe formmated retrieves the details information such as schema, table information, table parameters, storage information, storage desc params, table creation date, type of table etc.

11.How to skip header rows from a table in Hive?
Ans:
We can skip the headers by using the below commands.

create table table name
(
parameters
)
tblproperties("skip.header.line.count" = "1");

12.What is a hive operator? What are the different types of hive operators?

The HiveQL operators are operators facilite to perform arithmetic and relational operations. 

Types of operators are:
* Arithmetic ( A+B,A-B,A*B,A/B,A%B,A&B,A^B,A`B,A|B)
* Relational (A=B,A <> B, A !=B,A<B,A>B,A<=B.A>=B,A IS NULL, A IS NOT NULL )
* Logical ( A AND B, A && B, A OR B, A||B, NOT A, !A )
* Complex operators ( A[n], M[Key], S.x )

13.Explain about the Hive Built-In Functions.

Types of operators are:
* Arithmetic ( A+B,A-B,A*B,A/B,A%B,A&B,A^B,A`B,A|B,A LIKE B, A RLIKE B, A REGEXP B)
* Relational (A=B,A <> B, A !=B,A<B,A>B,A<=B.A>=B,A IS NULL, A IS NOT NULL )
* Logical ( A AND B, A && B, A OR B, A||B, NOT A, !A )
* Complex operators ( A[n], M[Key], S.x )

14. Write hive DDL and DML commands

DDL Commands - CREATE, SHOW, DESCRIBE, USE, DROP, ALTER, TRUNCATE;
DML Commands - LOAD, SELECT, INSERT,DELETE, UPDATE, EXPORT, IMPORT.

15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.

SORT BY: 
* Sort by is an local sorting.
* Explicitly we can mention the reduces to set the jobs.
* Data will be categorized into reducers and it sorts the data on each reducer locally.

ORDER BY:
* Order by arranges data globally.
* Entire data will be considered as a single reducer, even if we set the reducers manually.
* Limit can be used to minimize sort time.

DISTRIBUTE BY:

* DISTRIBUTE BY clause is used to distribute the input rows among reducers.
* It ensures that all rows for the same key columns are going to the same reducer.
* The DISTRIBUTE BY clause does not sort the data either at the reducer level or globally.
* Also, the same key values might not be placed next to each other in the output dataset.

CLUSTER BY:

* CLUSTER BY clause is a combination of DISTRIBUTE BY and SORT BY clauses together. That means the output of the CLUSTER BY clause is equivalent to the output of DISTRIBUTE BY + SORT BY clauses.
* The output of the CLUSTER BY clause is sorted at the reducer level. As a result, we can get N number of sorted output files where N is the number of reducers used in the query processing. 

16.Difference between "Internal Table" and "External Table" and Mention when to choose “Internal Table” and “External Table” in Hive?

Internal Table:

* Internal table is also known as managed table. It is the default table in hive without specifying as external keyword. 
* By default, an internal table will be created in a folder path similar to /user/hive/warehouse directory of HDFS. 
* We can override the default location by the location property during table creation.
* If we drop the managed table or partition, the table data and the metadata associated with that table will be deleted from the HDFS.

External Table:

* While creating the external table, External keyword will be used.
* Hive does not manage the data of the External table.
* External tables are stored outside the warehouse directory. They can access data stored in sources such as remote HDFS locations or Azure Storage Volumes.
* Whenever we drop the external table, then only the metadata associated with the table will get deleted, the table data remains untouched by Hive.

17.Where does the data of a Hive table get stored?

* Internal table data will be stored in Hive warehouse,  a folder path similar to /user/hive/warehouse directory of HDFS.
* External tables are stored outside the warehouse directory. They can access data stored in sources such as remote HDFS locations or Azure Storage Volumes.

18.Is it possible to change the default location of a managed table?

Yes, we can change the location of a table by s[ecifying the clause -Location while creating the table.
eg.
Create table employee
( id, int, name string, salary string )
row format delimited fields terminated by ','
location "/tmp/hive/data/weather';

19.What is a metastore in Hive? What is the default database provided by Apache Hive for metastore?

Ans: 
The Hive metastore is simply a relational database. It stores metadata related to the tables/schemas you create to easily query big data stored in HDFS. 
When you create a new Hive table, the information related to the schema (column names, data types) is stored in the Hive metastore relational database.
Derby is the default metastore in hive which supports only one user

20.Why does Hive not store metadata information in HDFS?

Ans - Hive stores metadata information in the metastore using RDBMS instead of HDFS. The reason for choosing RDBMS is to achieve low latency as HDFS read/write operations are time consuming processes.

21.What is a partition in Hive? And Why do we perform partitioning in Hive?

* It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data.
* Partitioning helps to reduce the latency and increase the query processing time.
* It increase the performance of the query while processing.

22.What is the difference between dynamic partitioning and static partitioning?

Static :
* We need to manually create each partition before inserting data into a partition
* We need to know all partitions in advance. So it is suitable for use cases where partitions are defined well ahead and are small in number.
eg: 
hive> Create TABLE sales ( id int, name string )
PARTITIONED BY (country="US") ;

Dynamic :
* When we don't know ,how any distinct values in a table, Partitions will be created dynamically based on input data to the table.
* Dynamic partitions are suitable when we have lot of partitions and we can not predict in advance new partitions ahead of time.

eg: 
hive> Create TABLE sales ( id int, name string )
PARTITIONED BY (country string);

23.How do you check if a particular partition exists?

We can able to check using the below query.
hive> show partitions tablename;

24.How can you stop a partition form being queried?

We can restrict by putting Hive into strict mode using the set hive. mapred. mode=strict command. In this mode, when users submit a query that would result in a full table scan.

25.Why do we need buckets? How Hive distributes the rows into buckets?

Bucketing in hive is the concept of breaking data down into ranges, which are known as buckets, to give extra structure to the data so it may be used for more efficient queries.
eg:
CREATE TABLE zipcodes(
RecordNumber int,
Country string,
City string,
Zipcode int)
PARTITIONED BY(state string)
CLUSTERED BY (Zipcode) INTO 32 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';

26.In Hive, how can you enable buckets?

Bucketing can be enabled using the command, set hive.enforce.bucketing = true;

27.How does bucketing help in the faster execution of queries?

Bucketing improves performance by shuffling and sorting data prior to downstream operations such as table joins. 
The tradeoff is the initial overhead due to shuffling and sorting, but for certain data transformations, this technique can improve performance by avoiding later shuffling and sorting.

28.How to optimise Hive Performance? Explain in very detail

Hive performance can be optimized by the below 7 types.
* Vectorization
* Partitioning
* Tex-Execution engine
* Cost Based
* Indexing
* Bucketing
* ORC file format.

29. What is the use of Hcatalog?

HCatalog is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce applications. 

30. Explain about the different types of join in Hive.

There are 4 types of optimized joins in hive.

1. Reduce side join: 
* Whole join will be happening from the reducer side.
* This join can be applied when both datasets are small.
* To enable this join perform the below command in hive shell

set hive.autoi.convert.join = False

2. Map side join or Broadcast join:

* set hive.auto.convert.join = true;

3. Bucket Map Join:

* This joins will be used when both datasets size is very huge.
* to enable this join, perform the below commands in hive shell.

set hive.auto.convert.join = true;
set hive.optimize.bucketmapjoin = true;

4. Sorted Merge Join:

In Hive, while each mapper reads a bucket from the first table and the corresponding bucket from the second table.
In SMB join. Basically, then we perform a merge sort join feature. Moreover, we mainly use it when there is no limit on file or partition or table join.

to perform this join, please execute the below commands in hive shell.

set hive.enforce.sortmergebucketjoin = false;
set hive.auto.convert.setmerge.join = true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

31.Is it possible to create a Cartesian join between 2 tables, using Hive?

Yes, possible. Cross join, also known as Cartesian product, is a way of joining multiple tables in which all the rows or tuples from one table are paired with the rows and tuples from another table.

32.Explain the SMB Join in Hive?

In Hive, while each mapper reads a bucket from the first table and the corresponding bucket from the second table.
In SMB join. Basically, then we perform a merge sort join feature. Moreover, we mainly use it when there is no limit on file or partition or table join.

to perform this join, please execute the below commands in hive shell.

set hive.enforce.sortmergebucketjoin = false;
set hive.auto.convert.setmerge.join = true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


33.What is the difference between order by and sort by which one we should 
use?

* Order by arrangees data globally.
* When we apply ORDER BY, whole data will be considered as a single reducer and it will sort the data as per the request ( Ascending or DEscending ).
* Entore data will come into a single reducer even if we set the reducers manually.

* Sort by arranges the data locally.
* Which means, data will be divided into a no of reducers and data will be sorted for each reducer.
* Dta will be sorted by the no of reducers automatically or else we can set the reducers manually.

set mapreduce.job.reduces = 3;

34.What is the usefulness of the DISTRIBUTED BY clause in Hive?

DISTRIBUTE BY clause is used to distribute the input rows among reducers. It ensures that all rows for the same key columns are going to the same reducer. So, if we need to partition the data on some key column, we can use the DISTRIBUTE BY clause in the hive queries.

35.How does data transfer happen from HDFS to Hive?.

* Using Insert Command. We can load data into a table using Insert command in two ways. One Using Values command and other is using queries.

INSERT OVERWRITE TABLE my_table_backup PARTITION (ds) SELECT * FROM my_table WHERE ds = ds;

* Using Load. You can load data into a hive table using Load statement in two ways.
LOAD DATA INPATH ‘hdfs_file’ INTO TABLE tablename; command, it looks like it is moving the hdfs_file to hive/warehouse dir.

36.Wherever (Different Directory) I run the hive query, it creates a new metastore_db, please explain the reason for it?

When running Hive in embedded mode, it creates a local metastore. When you run the query, it first checks whether a metastore already exists or not.
The property javax.jdo.option.ConnectionURL defined in the hive-site.xml has a default value jdbc: derby: databaseName=metastore_db; create=true.

37.What will happen in case you have not issued the command: ‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive?

If we do not set this property in Hive Session, we have to manually convey same information to Hive that, number of reduce tasks to be run (for example in our case, by using set mapred.reduce.tasks=32) 

38.Can a table be renamed in Hive?

Yes, we can rename the table using the below commands.

ALTER TABLE db_name.test_1 RENAME TO db_name.test_2;

39.Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)

ALTER TABLE h_table CHANGE COLUMN new_col INT BEFORE x_col ;

40.What is serde operation in HIVE?

The SerDe interface allows you to instruct Hive about how a record should be processed. 
A SerDe is a combination of a Serializer and a Deserializer. Hive uses SerDe (and FileFormat) to read and write the table's row.

41.Explain how Hive Deserializes and serialises the data?

*Serialization:

--Converting the data into the bytes is called serialization. We cannot read the serialized data.

The file will be stored in the different formats like , Parquet, ORC, AVRO etc..

create table table name ( values....., serde ) stored as parquet;


*Deserialization:

--Converting the serailized data ( bytes ) into its original format is called deserialization.

We can retrieve the data using select command. select * from table_name;

42.Write the name of the built-in serde in hive.

CSV Serde:
row format serde "org.apache.hadoop.hive.serde2.OpenCSVSerde"

JSON Serde:
row format serde "org.apache.hive.hcatalog.data.JsonSerde"

43.What is the need of custom Serde?.

Depending on the nature of data the user has, the inbuilt SerDe may not satisfy the format of the
data. SO users need to write their own java code to satisfy their data format requirements.


44.Can you write the name of a complex data type(collection data types) in 
Hive?

Complex data types are

Array, Map, Struct, Union.

45.Can hive queries be executed from script files? How?

Yes, script file will be executed in hive.

* Create a file with the format of .sql and save it in a location.
* open the file and run the queries and save it in a location.
* Run the command "hive -f path/" to execute the file.
* While executing the script, make sure that the entire path of the location of the Script file is present.

46.What are the default record and field delimiter used for hive text files?

The default record delimiter is − \n
And the filed delimiters are − \001,\002,\003

47.How do you list all databases in Hive whose name starts with s?

show databases like "s%";

48.What is the difference between LIKE and RLIKE operators in Hive?

LIKE is an operator similar to LIKE in SQL. We use LIKE to search for string with similar text.

E.g. user_name LIKE ‘%Smith’

RLIKE is an operator which used to set multiple conditions in a single function.

E.g : select * from table_name where name RLIKE '(JAI|SREE|RAM')'; 

which displays all the names matches with the string "JAI OR SREE OR RAM";

49.How to change the column data type in Hive?

Syntax - ALTER TABLE table_name CHANGE oldcolumnname new_column_name data_type;

ALTER TABLE employee CHANGE e_id e_id INT;

50.How will you convert the string ’51.2’ to a float value in the particular column?

SELECT col1, cast('51.2' as float) from sample_table;

51.What will be the result when you cast ‘abc’ (string) as INT?

select cast('abc' as int);

52.What does the following query do?
a. INSERT OVERWRITE TABLE employees
b. PARTITION (country, state)
c. SELECT ..., se.cnty, se.st
d. FROM staged_employees se;

Ans:
This will create a dynamic partitiong based on the country and state column from the employee table.

Data will be stored in the warehouse as a
"/country/state1"
"/Country/State2"
"/Country/State3"

53.Write a query where you can overwrite data in a new table from the existing table.

from sales_data_v1 insert overwrite table sales_data_v2 select *;
or 
insert overwrite table sales_data_v2 select * from sales_data_v1;

54.What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.

The maximum size of a string data type supported by Hive is 2 GB.

supports the binary format sequence files, ORC files, Avro data files, text files, Custom INPUTFORMAT and OUTPUTFORMAT and Parquet files. Sequence file: It is a splittable, compressible, and row-oriented file with a general binary format.

55. What File Formats and Applications Does Hive Support?

Hive Different File Formats:TextFile, SequenceFile, RCFile,Custom INPUTFORMAT and OUTPUTFORMAT, AVRO, ORC,Parquet. Apache Hive supports several familiar file formats used in Apache Hadoop.

56.How do ORC format tables help Hive to enhance its performance?

The ORC format improves query performance also by the way it stores data in a file. Data is stored in a columnar format and columns that are not needed in a query can be skipped, thus leading to better performance.

57.How can Hive avoid mapreduce while processing the query?

-->select * from table;
This query needs only read data from HDFS. So far neither requires any map or reduce phases.

-->select * from table where color in ('RED','WHITE','BLUE')
This query requires only a map only, there is no reduce phase. There is no aggregation function of any kind.
Here we are filtering to collect records that are RED, WHITE, or BLUE.

-->select count(1) from table;
This query requires only a reduce phase. No mapping required because we are counting all the records in the table.
If we want to count across elements then we will be adding a map phase prior to the reduce phase. See below:

58.What is view and indexing in hive?

Apache Hive View is a searchable object in a database which we can define by the query. However, we can not store data in the view. Still, some refer to as a view as “virtual tables”.

Indexing, we can say these are pointers to particular column name of a table. However, the user has to manually define the Hive index
Basically, we are creating the pointer to particular column name of the table, wherever we are creating Hive index.
By using the Hive index value created on the column name, any Changes made to the column present in tables are stored.

59.Can the name of a view be the same as the name of a hive table?

No, We can create the view as different from the actual table. It is not an mandatory to keep the view name as same as the table name.

60.What types of costs are associated in creating indexes on hive tables?

Basically, there is a processing cost in arranging the values of the column on which index is created since Indexes occupies.

61.Give the command to see the indexes on a table.

hive> Show index on table_name;

62. Explain the process to access subdirectories recursively in Hive queries.

To process directories recursively in Hive, we need to set below two commands in hive session. These two parameters work in conjunction.

hive> Set mapred.input.dir.recursive=true;
hive> Set hive.mapred.supports.subdirectories=true;
Now hive tables can be pointed to the higher level directory. This is suitable for a scenario where the directory structure is as following: /data/country/state/city

63. If you run a select * query in Hive, why doesn't it run MapReduce?

When we execute the select * query, it will directly fetch the data from the table and there is no action required for mapping and reducer. 
Unlikely, if we execute select count(*) query, it will check the entire table and reduce the table into a single line and return the results.

64.What are the uses of Hive Explode?

Explodes function takes inputs as an array and return the output as an each individual row.

query:
select explode (<array>) from table_name;

65. What is the available mechanism for connecting applications when we run Hive as a server?

There are following ways by which you can connect with the Hive Server:Thrift Client: Using thrift you can call hive commands from a various programming languages e.g. C++, Java, PHP,
Python and Ruby.JDBC Driver : It supports the Type 4 (pure Java) JDBC DriverODBC Driver: It supports ODBC protocol.

66. Can the default location of a managed table be changed in Hive?

Yes, we can change the location of a managed (internal ) table while creatng a table using the Location clause.

create table table_name 
( clumn_name d_types,

)
LOCATION '<hdfs_path>' 

67.What is the Hive ObjectInspector function?

A key concept when working with Generic UDF and UDAF is the ObjectInspector. In generic UDFs, all objects are passed around using the Object type.
Hive is structured this way so that all code handling records and cells is generic, and to avoid the costs of instantiating and deserializing objects when it's not needed.

68.What is UDF in Hive?

UDF refers to user defined functions which allows to create a custom functions to process records or groups of records.

69.Write a query to extract data from hdfs to hive.

Below are the steps to process the data from HDFS to Hive.

1. Create a folder/file in HDFS location. E.g: /user/hive/student.
2. Create a managed table in hive shell
3. load data from hdfs to newly created table using the below query.
load data inpath "hdfs:///user/hive/student/filename___" into table table_name;
4. data will be copied from the hdfs to the internal table and we can check it by executing the cmd: select * from table_name;

70.What is TextInputFormat and SequenceFileInputFormat in hive.

Hadoop's performance is drawn out when we work with a small number of files with big size rather than a large number of files with small size.
If the size of a file is smaller than the typical block size in Hadoop, we consider it as a small file. Due to this, a number of metadata increases which will become an overhead to the NameNode. To solve this problem sequence files are introduced in Hadoop. Sequence files act as a container to store the small files.

eg: write STORED AS SEQUENCEFILE at the end of create table statement.

TEXTFILE format is a famous input/output format used in Hadoop. In Hive if we define a table as TEXTFILE it can load data of from CSV (Comma Separated Values),delimited by Tabs, Spaces, and JSON data. 
This means fields in each record should be separated by comma or space or tab or it may be JSON(JavaScript Object Notation) data.

eg: write STORED AS TEXTFILE at the end of create table statement.

71.How can you prevent a large job from running for a long time in a hive?

This can be achieved by setting the MapReduce jobs to execute in strict mode set hive.mapred.mode=strict;
The strict mode ensures that the queries on partitioned tables cannot execute without defining a WHERE clause.

72.When do we use explode in Hive?

The explode function explodes an array to multiple rows. Returns a row-set with a single column (col), one row for each element from the array.

73.Can Hive process any type of data formats? Why? Explain in very detail

Yes, we can achieve this through a combination of different external tables. Because different SerDes with different specifications for how to read columns in the different files will be needed, you will need to create one external table per type of file (and table).

74.Whenever we run a Hive query, a new metastore_db is created. Why?

A local metastore is created when we run Hive in an embedded mode. Before creating, it checks whether the metastore exists or not, and this metastore property is defined in the configuration file, hive-site.xml. 
The property is:

javax.jdo.option.ConnectionURL
with the default value:
jdbc:derby:;databaseName=metastore_db;create=true


75.Can we change the of a column in a hive table? Write a complete query.

Yes, we can change the column data type in a table.

Syntax - ALTER TABLE table_name CHANGE oldcolumnname new_column_name data_type;

76.While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file ?

When we load data from local, we will use the LOCAL keyword in the statement. 

e.g: load data local inpath "file:///tmp/hive" into table_name;

When we load data from hdfs, we won't use LOCAL keyword and location path starts with "hdfs:///"
e.g: load data inpath "hdfs:///tmp/hive" into table_name;


77.What is the precedence order in Hive configuration?

There is a precedence hierarchy to setting properties. In the following list, lower numbers take precedence over higher numbers:
The Hive SET command
The command line -hiveconf option
hive-site.xml
hive-default.xml
hadoop-site.xml (or, equivalently, core-site.xml, hdfs-site.xml, and mapred-site.xml)
hadoop-default.xml (or, equivalently, core-default.xml, hdfs-default.xml, and mapred-default.xml)


78.Which interface is used for accessing the Hive metastore?

1. Go to your Azure Databricks cluster, select Apps, and then select Launch Web Terminal. 
Run the cmdlet cat /databricks/hive/conf/hive-site. xml . Metastore JDBC URL: Provide the connection URL value and define the connection to the URL of the Metastore database server.

2. WebHCat API web interface can be used for Hive commands

79.Is it possible to compress json in the Hive external table ?

Yes, we can reduce the JSON file size using the Gzip properties. 

Enable the below commands after creating the external table.

set mapred.output.compress=true;
set hive.exec.compress.output=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;
set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;

Then load the data into table.

80.What is the difference between local and remote metastores?

Local Metastore:

In local metastore configuration, the metastore service runs in the same JVM in which the Hive service is running and connects to a database running in a separate JVM, either on the same machine or on a remote machine.

Remote Metastore:

In the remote metastore configuration, the metastore service runs on its own separate JVM and not in the Hive service JVM. Other processes communicate with the metastore server using Thrift Network APIs. You can have one or more metastore servers in this case to provide more availability.

81.What is the purpose of archiving tables in Hive?

The use of Hadoop Archives is one approach to reducing the number of files in partitions. Hive has built-in support to convert files in existing partitions to a Hadoop Archive (HAR) so that a partition that may once have consisted of 100's of files can occupy just ~3 files 

82.What is DBPROPERTY in Hive?

The DB properties are nothing but mentioning the details about the database created by the user. Suppose the name of the user, the type of the database and the tables it has, the date on which the database is created etc. This makes the other user easy the recognize the database and use it according to the requirement.

83.Differentiate between local mode and MapReduce mode in Hive?

1. Local Mode - In Hive local mode, Map Reduce jobs related to Hive run locally on a user machine. This is the default mode in which Hadoop uses local file system.
2. MapReduce mode or distributed mode - Hive as well as Hadoop is running in a fully distributed mode. NameNode, DataNode, JobTracker, TaskTracker etc run on different machines in this mode.


























