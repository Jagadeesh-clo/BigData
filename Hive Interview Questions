1. What is the definition of Hive? What is the present version of Hive? 
Ans: Hive is a Data Warehouse system which is used to analyze the structured data and build on top of Hadoop. 
Hive mainly used to manage the data using SQL queries named as HQL ( Hive query language). It's main functionalities includes reading, writing and managing large datasets residing in distributed storage. 
Using Hive, we can skip the requirement of the traditional approach of writing complex MapReduce programs. Hive supports, DDL,DML and User defined functions.

Version -- Apache hive 3.1.2 ( released on april 2022 )

2. Is Hive suitable to be used for OLTP systems? Why?
Ans; No, hive does not support insert and update at row level. So it does not support OLTP system. 

3. How is HIVE different from RDBMS? Does hive support ACID 
transactions. If not then give the proper reason.

RDBMS is a subset of DBMS. A relational database refers to a database that stores data in a structured format using rows and columns and that structured form is known as table.
Hive is a data warehouse software system that provides data query and analysis. 
RDBMS used to maintain databases and hive used to maintain the warehouses. Schema is fixed in RDBMS and schema will vary in Hive.

Hive supports all ACID properties which enable atomicity of operations at the row level, which allows a Hive client to read from a partition or table and simultaneously, another Hive client can add rows to the same partition or table.

4. Explain the hive architecture and the different components of a Hive architecture?

Hive architecture is made up of below components.
* Hive Client ( Thrift Server, JDBC Server,ODBC server)
* Hive Services ( Hive Driver -->MetaStore )
* Processing and resource management ( MapReduce ) 
* HDFS - Distributed storage system.

5. Mention what Hive query processor does? And Mention what are the 
components of a Hive query processor?

Ans: Hive query processor convert graph of MapReduce jobs with the execution time framework. So that the jobs can be executed in the order of dependencies.

Components of a Hive Query Processor:

Parse and Semantic Analysis (ql/parse)
Metadata Layer (ql/metadata)
Type Interfaces (ql/typeinfo)
Sessions (ql/session)
Map/Reduce Execution Engine (ql/exec)
Plan Components (ql/plan)
Hive Function Framework (ql/udf)
Tools (ql/tools)
Optimizer (ql/optimizer)

6. What are the three different modes in which we can operate Hive?

Hive has three different modes.
1. Local Mode - In Hive local mode, Map Reduce jobs related to Hive run locally on a user machine. This is the default mode in which Hadoop uses local file system.
2. MapReduce mode or distributed mode - Hive as well as Hadoop is running in a fully distributed mode. NameNode, DataNode, JobTracker, TaskTracker etc run on different machines in this mode.
3. Pseudo distributed mode. - This is the mode used by developers to test the code before deploying to production. In this mode, all the daemons run on same virtual machine. With this mode, we can quickly write scripts and test on limited data sets.

 7. Features and Limitations of Hive.

Ans: 

Features of Hive:
* Open-source: Apache Hive is an open-source tool. We can use it free of cost.
* Query large datasets: Hive can query and manage huge datasets stored in Hadoop Distributed File System.
* Multiple-users: Multiple users can query the data using Hive Query Language simultaneously.
* Apache Hive supports partitioning and bucketing of data at the table level to improve performance.
* Apache Hive supports external tables. This allows us to process data without actually storing data in HDFS.

Limitations of Hive:
* Hive is not designed for the OLTP (Online transaction processing). We can use it for OLAP.
* It does not offer real-time queries.
* It provides limited subquery support.
* Latency of hive is generally high.
* Hive uses HIVE query language to query structure data which is easy to code.

8. How to create a Database in HIVE?

Ans: Inorder to create a database in hive, we should open the Hive shell.
* We will be using the HQL ( Hive query language) in hive shell.
Syntax: create database databasename;
eg: create database hive_class;

9. How to create a table in HIVE?
Ans: Open the hiveshell and enter the below code to create the tables.

Query: Use database hive_class;

create table sampletable1
(
id int,
name string,
location string
)
row format delimited
fields terminated by ",";

#loading data into the table:

load data local inpath "file:///path_____" into table sampletable1;

10.What do you mean by describe and describe extended and describe 
formatted with respect to database and table

Describe:
Describe statement gives the basic information such column names and it's data types of a table.
eg. describe sampletable1;

describe extended:
describe extended gives the detailed table information such as schema of the table,warehouse info, db info etc.

Describe formatted:

Describe formmated retrieves the details information such as schema, table information, table parameters, storage information, storage desc params, table creation date, type of table etc.

11.How to skip header rows from a table in Hive?
Ans:
We can skip the headers by using the below commands.

create table table name
(
parameters
)
tblproperties("skip.header.line.count" = "1");

12.What is a hive operator? What are the different types of hive operators?

The HiveQL operators are operators facilite to perform arithmetic and relational operations. 

Types of operators are:
* Arithmetic ( A+B,A-B,A*B,A/B,A%B,A&B,A^B,A`B,A|B)
* Relational (A=B,A <> B, A !=B,A<B,A>B,A<=B.A>=B,A IS NULL, A IS NOT NULL )
* Logical ( A AND B, A && B, A OR B, A||B, NOT A, !A )
* Complex operators ( A[n], M[Key], S.x )

13.Explain about the Hive Built-In Functions.

Types of operators are:
* Arithmetic ( A+B,A-B,A*B,A/B,A%B,A&B,A^B,A`B,A|B,A LIKE B, A RLIKE B, A REGEXP B)
* Relational (A=B,A <> B, A !=B,A<B,A>B,A<=B.A>=B,A IS NULL, A IS NOT NULL )
* Logical ( A AND B, A && B, A OR B, A||B, NOT A, !A )
* Complex operators ( A[n], M[Key], S.x )

14. Write hive DDL and DML commands

DDL Commands - CREATE, SHOW, DESCRIBE, USE, DROP, ALTER, TRUNCATE;
DML Commands - LOAD, SELECT, INSERT,DELETE, UPDATE, EXPORT, IMPORT.

15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.

SORT BY: 
* Sort by is an local sorting.
* Explicitly we can mention the reduces to set the jobs.
* Data will be categorized into reducers and it sorts the data on each reducer locally.

ORDER BY:
* Order by arranges data globally.
* Entire data will be considered as a single reducer, even if we set the reducers manually.
* Limit can be used to minimize sort time.

DISTRIBUTE BY:

* DISTRIBUTE BY clause is used to distribute the input rows among reducers.
* It ensures that all rows for the same key columns are going to the same reducer.
* The DISTRIBUTE BY clause does not sort the data either at the reducer level or globally.
* Also, the same key values might not be placed next to each other in the output dataset.

CLUSTER BY:

* CLUSTER BY clause is a combination of DISTRIBUTE BY and SORT BY clauses together. That means the output of the CLUSTER BY clause is equivalent to the output of DISTRIBUTE BY + SORT BY clauses.
* The output of the CLUSTER BY clause is sorted at the reducer level. As a result, we can get N number of sorted output files where N is the number of reducers used in the query processing. 

16.Difference between "Internal Table" and "External Table" and Mention when to choose “Internal Table” and “External Table” in Hive?

Internal Table:

* Internal table is also known as managed table. It is the default table in hive without specifying as external keyword. 
* By default, an internal table will be created in a folder path similar to /user/hive/warehouse directory of HDFS. 
* We can override the default location by the location property during table creation.
* If we drop the managed table or partition, the table data and the metadata associated with that table will be deleted from the HDFS.

External Table:

* While creating the external table, External keyword will be used.
* Hive does not manage the data of the External table.
* External tables are stored outside the warehouse directory. They can access data stored in sources such as remote HDFS locations or Azure Storage Volumes.
* Whenever we drop the external table, then only the metadata associated with the table will get deleted, the table data remains untouched by Hive.

17.Where does the data of a Hive table get stored?

* Internal table data will be stored in Hive warehouse,  a folder path similar to /user/hive/warehouse directory of HDFS.
* External tables are stored outside the warehouse directory. They can access data stored in sources such as remote HDFS locations or Azure Storage Volumes.

18.Is it possible to change the default location of a managed table?

Yes, we can change the location of a table by s[ecifying the clause -Location while creating the table.
eg.
Create table employee
( id, int, name string, salary string )
row format delimited fields terminated by ','
location "/tmp/hive/data/weather';

19.What is a metastore in Hive? What is the default database provided by Apache Hive for metastore?

Ans: 
The Hive metastore is simply a relational database. It stores metadata related to the tables/schemas you create to easily query big data stored in HDFS. 
When you create a new Hive table, the information related to the schema (column names, data types) is stored in the Hive metastore relational database.
Derby is the default metastore in hive which supports only one user

20.Why does Hive not store metadata information in HDFS?

Ans - Hive stores metadata information in the metastore using RDBMS instead of HDFS. The reason for choosing RDBMS is to achieve low latency as HDFS read/write operations are time consuming processes.

21.What is a partition in Hive? And Why do we perform partitioning in Hive?

* It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data.
* Partitioning helps to reduce the latency and increase the query processing time.
* It increase the performance of the query while processing.

22.What is the difference between dynamic partitioning and static partitioning?

Static :
* We need to manually create each partition before inserting data into a partition
* We need to know all partitions in advance. So it is suitable for use cases where partitions are defined well ahead and are small in number.
eg: 
hive> Create TABLE sales ( id int, name string )
PARTITIONED BY (country="US") ;

Dynamic :
* When we don't know ,how any distinct values in a table, Partitions will be created dynamically based on input data to the table.
* Dynamic partitions are suitable when we have lot of partitions and we can not predict in advance new partitions ahead of time.

eg: 
hive> Create TABLE sales ( id int, name string )
PARTITIONED BY (country string);

23.How do you check if a particular partition exists?

We can able to check using the below query.
hive> show partitions tablename;

24.How can you stop a partition form being queried?

We can restrict by putting Hive into strict mode using the set hive. mapred. mode=strict command. In this mode, when users submit a query that would result in a full table scan.

25.Why do we need buckets? How Hive distributes the rows into buckets?

Bucketing in hive is the concept of breaking data down into ranges, which are known as buckets, to give extra structure to the data so it may be used for more efficient queries.
eg:
CREATE TABLE zipcodes(
RecordNumber int,
Country string,
City string,
Zipcode int)
PARTITIONED BY(state string)
CLUSTERED BY (Zipcode) INTO 32 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';

26.In Hive, how can you enable buckets?

Bucketing can be enabled using the command, set hive.enforce.bucketing = true;

27.How does bucketing help in the faster execution of queries?

Bucketing improves performance by shuffling and sorting data prior to downstream operations such as table joins. 
The tradeoff is the initial overhead due to shuffling and sorting, but for certain data transformations, this technique can improve performance by avoiding later shuffling and sorting.

28.How to optimise Hive Performance? Explain in very detail

Hive performance can be optimized by the below 7 types.
* Vectorization
* Partitioning
* Tex-Execution engine
* Cost Based
* Indexing
* Bucketing
* ORC file format.

29. What is the use of Hcatalog?

HCatalog is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce applications. 

30. Explain about the different types of join in Hive.

There are 4 types of optimized joins in hive.

1. Reduce side join: 
* Whole join will be happening from the reducer side.
* This join can be applied when both datasets are small.
* To enable this join perform the below command in hive shell

set hive.autoi.convert.join = False

2. Map side join or Broadcast join:

* set hive.auto.convert.join = true;

3. Bucket Map Join:

* This joins will be used when both datasets size is very huge.
* to enable this join, perform the below commands in hive shell.

set hive.auto.convert.join = true;
set hive.optimize.bucketmapjoin = true;

4. Sorted Merge Join:

In Hive, while each mapper reads a bucket from the first table and the corresponding bucket from the second table.
In SMB join. Basically, then we perform a merge sort join feature. Moreover, we mainly use it when there is no limit on file or partition or table join.

to perform this join, please execute the below commands in hive shell.

set hive.enforce.sortmergebucketjoin = false;
set hive.auto.convert.setmerge.join = true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

31.Is it possible to create a Cartesian join between 2 tables, using Hive?

Yes, possible. Cross join, also known as Cartesian product, is a way of joining multiple tables in which all the rows or tuples from one table are paired with the rows and tuples from another table.

32.Explain the SMB Join in Hive?

In Hive, while each mapper reads a bucket from the first table and the corresponding bucket from the second table.
In SMB join. Basically, then we perform a merge sort join feature. Moreover, we mainly use it when there is no limit on file or partition or table join.

to perform this join, please execute the below commands in hive shell.

set hive.enforce.sortmergebucketjoin = false;
set hive.auto.convert.setmerge.join = true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


33.What is the difference between order by and sort by which one we should 
use?

* Order by arrangees data globally.
* When we apply ORDER BY, whole data will be considered as a single reducer and it will sort the data as per the request ( Ascending or DEscending ).
* Entore data will come into a single reducer even if we set the reducers manually.

* Sort by arranges the data locally.
* Which means, data will be divided into a no of reducers and data will be sorted for each reducer.
* Dta will be sorted by the no of reducers automatically or else we can set the reducers manually.

set mapreduce.job.reduces = 3;

34.What is the usefulness of the DISTRIBUTED BY clause in Hive?

DISTRIBUTE BY clause is used to distribute the input rows among reducers. It ensures that all rows for the same key columns are going to the same reducer. So, if we need to partition the data on some key column, we can use the DISTRIBUTE BY clause in the hive queries.

35.How does data transfer happen from HDFS to Hive?.

* Using Insert Command. We can load data into a table using Insert command in two ways. One Using Values command and other is using queries.

INSERT OVERWRITE TABLE my_table_backup PARTITION (ds) SELECT * FROM my_table WHERE ds = ds;

* Using Load. You can load data into a hive table using Load statement in two ways.
LOAD DATA INPATH ‘hdfs_file’ INTO TABLE tablename; command, it looks like it is moving the hdfs_file to hive/warehouse dir.

36.Wherever (Different Directory) I run the hive query, it creates a new metastore_db, please explain the reason for it?

When running Hive in embedded mode, it creates a local metastore. When you run the query, it first checks whether a metastore already exists or not.
The property javax.jdo.option.ConnectionURL defined in the hive-site.xml has a default value jdbc: derby: databaseName=metastore_db; create=true.

37.What will happen in case you have not issued the command: ‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive?

If we do not set this property in Hive Session, we have to manually convey same information to Hive that, number of reduce tasks to be run (for example in our case, by using set mapred.reduce.tasks=32) 

38.Can a table be renamed in Hive?

Yes, we can rename the table using the below commands.

ALTER TABLE db_name.test_1 RENAME TO db_name.test_2;

39.Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)

ALTER TABLE h_table CHANGE COLUMN new_col INT BEFORE x_col ;

40.What is serde operation in HIVE?

The SerDe interface allows you to instruct Hive about how a record should be processed. 
A SerDe is a combination of a Serializer and a Deserializer. Hive uses SerDe (and FileFormat) to read and write the table's row.

41.Explain how Hive Deserializes and serialises the data?

*Serialization:

--Converting the data into the bytes is called serialization. We cannot read the serialized data.

The file will be stored in the different formats like , Parquet, ORC, AVRO etc..

create table table name ( values....., serde ) stored as parquet;


*Deserialization:

--Converting the serailized data ( bytes ) into its original format is called deserialization.

We can retrieve the data using select command. select * from table_name;

42.Write the name of the built-in serde in hive.

CSV Serde:
row format serde "org.apache.hadoop.hive.serde2.OpenCSVSerde"

JSON Serde:
row format serde "org.apache.hive.hcatalog.data.JsonSerde"

43.What is the need of custom Serde?.

Depending on the nature of data the user has, the inbuilt SerDe may not satisfy the format of the
data. SO users need to write their own java code to satisfy their data format requirements.


44.Can you write the name of a complex data type(collection data types) in 
Hive?

Complex data types are

Array, Map, Struct, Union.

45.Can hive queries be executed from script files? How?

Yes, script file will be executed in hive.

* Create a file with the format of .sql and save it in a location.
* open the file and run the queries and save it in a location.
* Run the command "hive -f path/" to execute the file.
* While executing the script, make sure that the entire path of the location of the Script file is present.

46.What are the default record and field delimiter used for hive text files?

The default record delimiter is − \n
And the filed delimiters are − \001,\002,\003

47.How do you list all databases in Hive whose name starts with s?

show databases like "s%";

48.What is the difference between LIKE and RLIKE operators in Hive?

LIKE is an operator similar to LIKE in SQL. We use LIKE to search for string with similar text.

E.g. user_name LIKE ‘%Smith’

RLIKE is an operator which used to set multiple conditions in a single function.

E.g : select * from table_name where name RLIKE '(JAI|SREE|RAM')'; 

which displays all the names matches with the string "JAI OR SREE OR RAM";

49.How to change the column data type in Hive?

Syntax - ALTER TABLE table_name CHANGE oldcolumnname new_column_name data_type;

ALTER TABLE employee CHANGE e_id e_id INT;















