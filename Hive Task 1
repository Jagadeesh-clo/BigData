Q1: Will the reducer work or not if you use “Limit 1” in any HiveQL query?
Ans: Reducer won't work if we use the "limit" in Hive query.

Q2: Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
Ans: The default metastore configuration allows only one hive session at a time. It won't allow multiple clients to access it.

Q3: Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
Ans:

#1. Create a partitioned table:

CREATE TABLE transaction_partitioned (cust_id INT, amount FLOAT, country STRING) PARTITIONED BY (month STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; 

#2. Enable dynamic partitioning in Hive:

SET hive.exec.dynamic.partition = true;

SET hive.exec.dynamic.partition.mode = nonstrict;

#3. Transfer the data from the non – partitioned table into the newly created partitioned table:

INSERT OVERWRITE TABLE partitioned_transaction PARTITION (month) SELECT cust_id, amount, country, month FROM transaction_details;

Now, we can perform the query using the partitioned table and we can decrease the query time. 


Q4: How can you add a new partition for the month December in the above partitioned table?

#creating a static table:

create table transaction_details_static                                                                                                     
    > (                                                                                                                                       
    > cust_id int,                                                                                                                        
    > amount float,                                                                                                                    
    > country STRING)                                                                                                                             
    >)                                                                                                                                       
    > partitioned by (month string);

#loading data to local
insert overwrite table transaction_details_static partition(month = 'December') select cust_id, amount float, country string from transaction_details where month = 'December';

Q5: I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?

Ans: The error may be bacause of not setting the properties for partitioning. Execute the below query to set the properties.
hive> SET hive.exec.dynamic.partition=true;
hive> set hive.exec.dynamic.partition.mode=non-strict;

Q6: Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries: id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?

Ans: 
#query - row format serde "org.apache.hadoop.hive.serde2.OpenCSVSerde"

create table sample_table
( Id int,
first_name string,
last_name string,
email string,
gender string,
ip_address string
)
row format serde "org.apache.hadoop.hive.serde2.OpenCSVSerde"
with serdeproperties (
"separatorChar" = ",",
"quoteChar" = "\"",
"escapeChar" = "\\"
)
stored as textfile
tblproperties("skip.header.line.count"="1");

#loading Data
load data local inpath "file:///tmp/sample.csv" into table sample_table;

Q7: Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
Ans:

This can be achieved by using a sequence file/

#1 create a sample table

create table details 
( id int,
name string,
e-mail string,
country string
)
row format delimited
fields terminated by ",";

#loading data 
load data inpath "hdfs://path...." into table details;

#2. Creating a sequence file

create table seq_file
(
id int,
name string,
e-mail string,
country string
)
row format delimited
fields terminated by ","
stored as sequencefile;

#loading data
insert overwrite table seq_file select * from details;

#hence one single small file will be created and other small files will be eliminated. Therefore we can execute the queries without degrading it's performance.


Q8: LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;  #The following statement failed to execute. What can be the cause?

Ans: 
#1. The cause may be the file location is incorrect.
#2. File may be stored in local not in hdfs. use query 

hdfs dfs -copyFromLocal /tmp/hive_class/file /user/hive/;

Q9: Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?

Yes, we can add the nodes by following the below steps:

Step 1: Take a new system and create a new username and password for the user.
Step 2: Install SSH and with the master node setup SSH connections
Step 3: Add ssh public_rsa id key to the authorized keys file
Step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:
Step 5: Start the DataNode on a new node
Step 6: Login to the new node like suhadoop or:














